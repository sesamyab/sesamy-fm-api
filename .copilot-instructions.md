# Copilot Instructions for Sesamy FM API

## Project Overview

This is a Cloudflare Workers project that provides a podcast management API with audio encoding, transcription, and media processing capabilities.

## Important Project Structure Notes

### Container Setup

- **USE `container_src/` folder** - This is the correct directory for the encoding container
- **DO NOT create a `workers/` folder** - This is incorrect for this project
- The encoding container is in `container_src/` with its own `package.json` and `index.js`
- The main worker code is in `src/` directory

### Key Directories

```
src/                    # Main Cloudflare Worker source code
container_src/          # Encoding container (Durable Object) - USE THIS
data/                   # SQLite database files
drizzle/               # Database migrations
test/                  # Test files
examples/              # Example scripts
```

### Architecture Components

#### Cloudflare Worker (src/)

- Main API endpoints in `src/app.ts`
- Worker entry point in `src/worker.ts`
- Task processing system in `src/tasks/`
- Audio processing in `src/audio/`
- Database schema in `src/database/schema.ts`

#### Encoding Container (container_src/)

- Durable Object for FFmpeg audio encoding
- Runs in a separate container with FFmpeg installed
- Handles streaming progress updates
- **This is NOT a separate worker - it's a Durable Object container**

#### Key Services

- **TaskService** (`src/tasks/service.ts`) - Core task processing system
- **AudioService** (`src/audio/service.ts`) - Audio handling and R2 integration
- **EpisodeService** (`src/episodes/service.ts`) - Episode management
- **ShowService** (`src/shows/service.ts`) - Podcast show management

### Task Types

The system supports these task types:

- `transcribe` - Audio transcription using Cloudflare AI (Whisper)
- `encode` - Audio encoding using FFmpeg in the container
- `audio_preprocess` - Audio preprocessing (32kbps mono conversion)
- `publish` - Episode publishing (stub)
- `notification` - Notification handling (stub)

### Database

- Uses D1 (SQLite) for data storage
- Drizzle ORM for database operations
- R2 for media file storage

### Deployment

- Deploy with `npx wrangler deploy`
- Container is deployed as part of the same project
- Environment variables configured in `wrangler.toml`

## Common Patterns

### Creating Tasks

```typescript
await taskService.createTask("encode", {
  episodeId: "episode-123",
  audioUrl: "https://example.com/audio.mp3",
  outputFormat: "mp3",
  bitrate: 128,
});
```

### File Storage

- Original files stored in R2 under `episodes/{episodeId}/original/`
- Encoded files stored under `episodes/{episodeId}/encoded/`
- Transcripts stored under `transcripts/{episodeId}/`
- Uses signed URLs for secure access

### Testing Endpoints

- `/test/encode` - Test encoding without creating episodes
- `/test/transcribe` - Test transcription
- `/test/audio-preprocess` - Test audio preprocessing

## Development Guidelines

1. **Container Code**: Always modify `container_src/` for encoding-related changes
2. **Worker Code**: Main API logic goes in `src/`
3. **Database Changes**: Use Drizzle migrations in `drizzle/`
4. **New Features**: Follow the existing service/repository pattern
5. **Task Processing**: Extend TaskService for new task types

## Important Notes

- The encoding container uses FFmpeg and requires specific dependencies
- Transcription uses Cloudflare AI Workers (Whisper model)
- All media files use R2 with signed URLs for security
- Task system supports both immediate queue processing and batch processing
- Progress tracking is built into the task system

## Cloudflare Workflows Development Guidelines

### ‚ùå CRITICAL: NEVER Use Console Logs Inside Workflow Steps

**NEVER use `console.log()` statements inside Cloudflare Workflow step functions (`step.do()` callbacks).**

Console logs inside workflow steps will:

- Interfere with JSON serialization
- Cause truncated or corrupted step outputs
- Break workflow data flow
- Not be visible in production anyway

```typescript
// ‚ùå WRONG - Console logs inside step functions break JSON output
const result = await step.do("step-name", async () => {
  console.log("This will break JSON output!"); // DON'T DO THIS
  return { data: "value" };
});

// ‚úÖ CORRECT - Log after step completion
const result = await step.do("step-name", async () => {
  return { data: "value" };
});
console.log("Step completed:", result); // Log here instead
```

### üêõ Workflow Debugging Best Practices

#### 1. Include Debug Info in Return Objects

```typescript
return {
  success: true,
  data: processedData,
  debugInfo: {
    processingTime: Date.now() - startTime,
    itemsProcessed: items.length,
    debugUrls: signedUrls, // Always include signed URLs for R2 files
  },
};
```

#### 2. Include Debug Context in Error Messages

```typescript
try {
  // Processing logic
} catch (error) {
  const debugContext = {
    chunkIndex: chunk.index,
    r2Key: chunk.r2Key,
    debugUrl: signedUrl,
    timestamp: new Date().toISOString(),
  };

  throw new Error(
    `Processing failed for chunk ${chunk.index}. ` +
      `Debug info: ${JSON.stringify(debugContext)}. ` +
      `Original error: ${error}`
  );
}
```

#### 3. Generate Signed URLs for All R2 Files

Always include signed download URLs in workflow outputs for R2 objects:

```typescript
// Generate debug URLs for files
const debugUrl = await generateSignedDownloadUrl(env, r2Key, 3600);
return {
  r2Key,
  size,
  debugUrl: debugUrl.url, // Include for easy debugging access
};
```

#### 4. Helper Methods Should Return Data, Not Log

```typescript
// ‚ùå WRONG - Helper method with console.log
private async generateDebugUrls(items: Item[]): Promise<DebugUrl[]> {
  const urls = await Promise.all(/* ... */);
  console.log(`Generated ${urls.length} debug URLs`); // BREAKS WORKFLOW
  return urls;
}

// ‚úÖ CORRECT - Pure data return
private async generateDebugUrls(items: Item[]): Promise<DebugUrl[]> {
  const urls = await Promise.all(/* ... */);
  return urls; // Return structured data only
}
```

### üöÄ Why These Rules Matter

1. **Console logs inside workflow steps** interfere with JSON serialization
2. **Workflows are serverless** - console output may not be accessible
3. **Error messages are the primary debugging tool** in production
4. **Signed URLs provide immediate file access** for debugging
5. **Structured returns enable programmatic debugging**

Remember: **Workflow debugging relies on structured data returns and error context, not console output!**

```

```
